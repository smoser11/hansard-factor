{
    "contents" : "library(glmnet)\n\n## input: datP.use  eProd testrate = 0.2\ndata = datP.use\nprod = log(eProd)\nper.in = 0.8\nselect = TRUE\nset.seed(1977)\n\nsamp<-sample(1:nrow(data),nrow(data)*per.in)\n\neProd.in<-prod[samp]\ndat.use.in<-data[samp,]\neProd.out<-prod[-samp]\ndat.use.out<-data[-samp,]\n\npredDrilling <- function(data, prod, samp){\n  \n  eProd.in<-prod[samp]\n  dat.use.in<-data[samp,]\n  eProd.out<-prod[-samp]\n  dat.use.out<-data[-samp,]\n\n  vars<-varSelect1(eProd.in,dat.use.in,30)\n\n\n  is.better<-c()\n  m.i1<-lm(eProd.in~vars[,1])\n  for(i in 2:ncol(vars)){\n    m.i0<-m.i1\n    m.i1<-lm(eProd.in~vars[,1:i])\n    test<-anova(m.i0,m.i1)\n    if(is.na(test$\"Pr(>F)\"[2])==TRUE){\n      is.better<-c(is.better,0)\n    }else{\n      if(test$\"Pr(>F)\"[2]>0.05){\n        is.better<-c(is.better,0)\n      }else{\n        is.better<-c(is.better,1)\n      }\n    }#end if/else\n  }#end for\n  use.vars<-max(which(is.better==1))+1\n  vars<-vars[,1:use.vars]\n  \n  m<-lm(eProd.in~vars)\n  \n  \n  rm.var<-which(summary(m)$coefficients[,4]>0.05)\n  if(length(rm.var)>0){\n    vars<-vars[,-(rm.var-1)]\n  }\n  \n  w.in<-eProd.in/sum(eProd.in)\n  m<-lm(eProd.in~vars, weights=w.in)\n  \n  # training error and test error\n  pred.in <- predict.prod1(dat.use.in,m,colnames(vars))\n  trainingerror = t(eProd.in - pred.in)%*% (eProd.in-pred.in)/length(eProd.in)\n  pred.out<-predict.prod1(dat.use.out,m,colnames(vars))\n  testingerror = t(eProd.out - pred.out)%*% (eProd.out-pred.out)/length(eProd.out)\n  \n  #pred<-predict.prod(datP.use,m,colnames(vars),NBreaks=10,filename=bm)\n  error = c(trainingerror, testingerror)\n  return(error)\n}\n###### glmnet model #######\n## eProd.in<-prod[samp]\n## dat.use.in<-data[samp,]\n\nmypred <- function(data, prod, samp){\n  \n  eProd.in<-prod[samp]\n  dat.use.in<-data[samp,]\n  eProd.out<-prod[-samp]\n  dat.use.out<-data[-samp,]\n  \n  \nx = as.matrix(dat.use.in)\ny = as.matrix(eProd.in)\n\nw.in = y/sum(y)\n\ncv1 = cv.glmnet(x, y , weights = w.in, lambda = 10^(seq(-2,2,by=0.1)), alpha = 1)\nfit1 = glmnet(x,y,weights = w.in, lambda = cv1$lambda.min, alpha = 1)\n\n#cv2 = cv.glmnet(x, y , weights = w.in, lambda = 10^(seq(-2,2,by=0.1)), alpha = 0.5)\n#fit2 = glmnet(x,y,weights = w.in, lambda = cv2$lambda.min, alpha = 0.5)\n\n#cv3 = cv.glmnet(x, y , weights = w.in, lambda = 10^(seq(-2,2,by=0.1)), alpha = 0)\n#fit3 = glmnet(x,y,weights = w.in, lambda = cv3$lambda.min, alpha = 0)\n\n\n#prod1 = predict(fit1, as.matrix(datP.use))\n\n#prod2 = predict(fit2, as.matrix(datP.use))\n\n# training and testing error\n\n#pred.in = predict(fit3, as.matrix(dat.use.in))\n#pred.out = predict(fit3, as.matrix(dat.use.out))\n  \npred.in = predict(fit1, as.matrix(dat.use.in))\npred.out = predict(fit1, as.matrix(dat.use.out))\ntrainingerror = t(eProd.in - pred.in)%*% (eProd.in-pred.in)/length(eProd.in)\ntestingerror = t(eProd.out - pred.out)%*% (eProd.out-pred.out)/length(eProd.out)\n\nerror = c(trainingerror, testingerror)\n\nreturn(error)\n}\ncompare = rep(0,4)\ncompare[1] = t(pred$pred - prod) %*% (pred$pred - prod)\ncompare[2] = t(prod1 - prod) %*% (prod1 - prod)\ncompare[3] = t(prod2 - prod) %*% (prod2 - prod)\ncompare[4] = t(prod3 - prod) %*% (prod3 - prod)\n\ncompare <- function(data, prod, iterN){\n  errorM = matrix(0, iterN, 4)\n  for(iter in 1:iterN){\n    set.seed(1976+iter)\n    samp<-sample(1:nrow(data),nrow(data)*per.in)\n    error1 = predDrilling(data, prod, samp)\n    errorM[iter,1:2] = error1\n    error2 = mypred(data, prod, samp)\n    errorM[iter,3:4] = error2\n       \n  }\n  \n  return(errorM)\n    \n}\n  \n  \n  \n  \n  \n  \n  ",
    "created" : 1366323091584.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "672746147",
    "id" : "1001A640",
    "lastKnownWriteTime" : 1363822282,
    "path" : "~/Documents/course_project/myproject/myRegression.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}